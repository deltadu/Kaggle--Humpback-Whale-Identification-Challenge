{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdu12\\Anaconda3\\envs\\tf\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imutils import paths\n",
    "\n",
    "from subprocess import check_output\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = glob(\"C:/Users/jdu12/Desktop/humpback/train/*jpg\")\n",
    "test_images = glob(\"C:/Users/jdu12/Desktop/humpback/test/*jpg\")\n",
    "df = pd.read_csv(\"C:/Users/jdu12/Desktop/humpback/train.csv\")\n",
    "\n",
    "#df[\"Image\"] = df[\"Image\"].map( lambda x : \"C:/Users/jdu12/Desktop/humpback/input/train/\"+x)\n",
    "#ImageToLabelDict = dict( zip( df[\"Image\"], df[\"Id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single image to np array\n",
    "def get_image(img_path):\n",
    "    img = load_img(img_path, target_size=(224, 224))#.convert('L')\n",
    "    img = img_to_array(img)\n",
    "    #img = preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9850\n",
      "(9850, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# accquire images' paths\n",
    "trainPaths = list(paths.list_images('C:/Users/jdu12/Desktop/humpback/train/'))\n",
    "print(len(trainPaths))\n",
    "train = np.array([get_image(img_path) for img_path in trainPaths])\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = load_img(trainPaths[0], target_size=(299, 299))\n",
    "#a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels):\n",
    "    one_hot = []\n",
    "    seen_id = set()\n",
    "    id2class = dict()\n",
    "    counter = 0\n",
    "    for id in labels:\n",
    "        if id not in seen_id:\n",
    "            seen_id.add(id)\n",
    "            one_hot.append(counter)\n",
    "            id2class[id] = counter\n",
    "            counter += 1\n",
    "        else:\n",
    "            one_hot.append(id2class[id])\n",
    "    one_hot = to_categorical(one_hot, num_classes = 4251)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['Id']\n",
    "labels = to_one_hot(labels)\n",
    "#print(labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and val\n",
    "# The dataset is super unbalanced, as there are many classes that contains only 1 image\n",
    "# As a result, train/val data cannot be split before generating more data by augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data generator\n",
    "#use of an image generator for preprocessing and data augmentation\n",
    "#train = train.reshape( (-1,299,299,1))\n",
    "input_shape = train[0].shape\n",
    "x_train = train.astype(\"float32\")\n",
    "y_train = labels\n",
    "\n",
    "# define data generator\n",
    "image_gen = ImageDataGenerator(\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=.15,\n",
    "    height_shift_range=.15,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire the model\n",
    "model = keras.applications.densenet.DenseNet121(include_top=True, weights=None, classes=4251)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "307/307 [==============================] - 120s 390ms/step - loss: 5.6699 - acc: 0.1038\n",
      "Epoch 2/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 4.7876 - acc: 0.1285\n",
      "Epoch 3/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 4.1200 - acc: 0.1677\n",
      "Epoch 4/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 3.5708 - acc: 0.2236\n",
      "Epoch 5/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 3.0354 - acc: 0.3207\n",
      "Epoch 6/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 2.6025 - acc: 0.3915\n",
      "Epoch 7/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 2.2354 - acc: 0.4653\n",
      "Epoch 8/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 1.9268 - acc: 0.5198\n",
      "Epoch 9/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 1.7021 - acc: 0.5707\n",
      "Epoch 10/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 1.4531 - acc: 0.6308\n",
      "Epoch 11/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 1.3018 - acc: 0.6556\n",
      "Epoch 12/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 1.1334 - acc: 0.6965\n",
      "Epoch 13/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 1.0285 - acc: 0.7198\n",
      "Epoch 14/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.9161 - acc: 0.7497\n",
      "Epoch 15/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.8248 - acc: 0.7764\n",
      "Epoch 16/200\n",
      "307/307 [==============================] - 105s 341ms/step - loss: 0.7503 - acc: 0.7961\n",
      "Epoch 17/200\n",
      "307/307 [==============================] - 106s 344ms/step - loss: 0.7121 - acc: 0.8045\n",
      "Epoch 18/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 0.6415 - acc: 0.8217\n",
      "Epoch 19/200\n",
      "307/307 [==============================] - 105s 341ms/step - loss: 0.5848 - acc: 0.8348\n",
      "Epoch 20/200\n",
      "307/307 [==============================] - 106s 344ms/step - loss: 0.5728 - acc: 0.8359\n",
      "Epoch 21/200\n",
      "307/307 [==============================] - 105s 344ms/step - loss: 0.5666 - acc: 0.8372\n",
      "Epoch 22/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.5093 - acc: 0.8573\n",
      "Epoch 23/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.4835 - acc: 0.8618\n",
      "Epoch 24/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.4632 - acc: 0.8714\n",
      "Epoch 25/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.4524 - acc: 0.8718\n",
      "Epoch 26/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 0.4268 - acc: 0.8817\n",
      "Epoch 27/200\n",
      "307/307 [==============================] - 106s 344ms/step - loss: 0.3837 - acc: 0.8941\n",
      "Epoch 28/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.3958 - acc: 0.8906\n",
      "Epoch 29/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.3815 - acc: 0.8918\n",
      "Epoch 30/200\n",
      "307/307 [==============================] - 106s 344ms/step - loss: 0.3740 - acc: 0.8980\n",
      "Epoch 31/200\n",
      "307/307 [==============================] - 105s 344ms/step - loss: 0.3881 - acc: 0.8932\n",
      "Epoch 32/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.3692 - acc: 0.8983\n",
      "Epoch 33/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 0.3309 - acc: 0.9100\n",
      "Epoch 34/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 0.3047 - acc: 0.9106\n",
      "Epoch 35/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 0.2898 - acc: 0.9210\n",
      "Epoch 36/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.3054 - acc: 0.9195\n",
      "Epoch 37/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.3057 - acc: 0.9174\n",
      "Epoch 38/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.2934 - acc: 0.9205\n",
      "Epoch 39/200\n",
      "307/307 [==============================] - 105s 344ms/step - loss: 0.2975 - acc: 0.9164\n",
      "Epoch 40/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.2734 - acc: 0.9300\n",
      "Epoch 41/200\n",
      "307/307 [==============================] - 106s 344ms/step - loss: 0.2739 - acc: 0.9231\n",
      "Epoch 42/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.2755 - acc: 0.9245\n",
      "Epoch 43/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.3036 - acc: 0.9155\n",
      "Epoch 44/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 0.2706 - acc: 0.9279\n",
      "Epoch 45/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 0.2552 - acc: 0.9312\n",
      "Epoch 46/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 0.2368 - acc: 0.9344\n",
      "Epoch 47/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 0.2326 - acc: 0.9343\n",
      "Epoch 48/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.2430 - acc: 0.9351\n",
      "Epoch 49/200\n",
      "307/307 [==============================] - 105s 342ms/step - loss: 0.2278 - acc: 0.9399\n",
      "Epoch 50/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.2270 - acc: 0.9401\n",
      "Epoch 51/200\n",
      "307/307 [==============================] - 106s 344ms/step - loss: 0.2345 - acc: 0.9399\n",
      "Epoch 52/200\n",
      "307/307 [==============================] - 106s 344ms/step - loss: 0.2120 - acc: 0.9463\n",
      "Epoch 53/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.2119 - acc: 0.9458\n",
      "Epoch 54/200\n",
      "307/307 [==============================] - 105s 343ms/step - loss: 0.2332 - acc: 0.9386\n",
      "Epoch 55/200\n",
      "186/307 [=================>............] - ETA: 41s - loss: 0.2199 - acc: 0.9370"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 200\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_dir = 'C:/Users/jdu12/Desktop/humpback/saved_model/DenseNet/'\n",
    "routine_dir = model_dir + \"routine-{epoch:02d}-{acc:.2f}.hdf5\"\n",
    "routine_save = ModelCheckpoint(routine_dir, monitor='acc', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=10)\n",
    "model.fit_generator(image_gen.flow(x_train, y_train, batch_size=batch_size),\n",
    "          steps_per_epoch=  x_train.shape[0]//batch_size,\n",
    "          epochs=epochs, verbose=1, callback=[routine_save])\n",
    "          #class_weight=class_weight_dic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('C:/Users/jdu12/Desktop/humpback/saved_model/DenseNet/' + 'first_DenseNet.hdf5')\n",
    "#model.save(model_dir + 'first_DenseNet.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing and generating submission file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
