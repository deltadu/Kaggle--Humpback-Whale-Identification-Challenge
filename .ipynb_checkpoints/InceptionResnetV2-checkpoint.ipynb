{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdu12\\Anaconda3\\envs\\tf\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imutils import paths\n",
    "\n",
    "from subprocess import check_output\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.applications.densenet import preprocess_input\n",
    "#what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single image to np array\n",
    "def get_image(img_path):\n",
    "    img = load_img(img_path, target_size=(299, 299))\n",
    "    img = img_to_array(img)\n",
    "    img = img/255\n",
    "    return img\n",
    "\n",
    "def to_one_hot(labels):\n",
    "    id2class = dict()\n",
    "    class2id = dict()      ##!!! class(integer) to id(whale class)\n",
    "    num_labels = []\n",
    "    counter = 0\n",
    "    for id in labels:\n",
    "        if id not in id2class.keys():\n",
    "            num_labels.append(counter)\n",
    "            id2class[id] = counter\n",
    "            class2id[counter] = id\n",
    "            counter += 1\n",
    "        else:\n",
    "            num_labels.append(id2class[id])\n",
    "    one_hot_labels = to_categorical(num_labels, num_classes = 4251)\n",
    "    return one_hot_labels, class2id, id2class, num_labels\n",
    "\n",
    "\n",
    "# define data generator\n",
    "#use of an image generator for preprocessing and data augmentation\n",
    "image_gen = ImageDataGenerator(\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    #rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=.15,\n",
    "    height_shift_range=.15,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/jdu12/Desktop/humpback/train.csv\")\n",
    "\n",
    "labels = df['Id']\n",
    "labels, class2id, id2class, num_labels = to_one_hot(labels)\n",
    "\n",
    "# generate class weights\n",
    "from sklearn.utils import class_weight\n",
    "cw = class_weight.compute_class_weight('balanced', np.arange(len(class2id)), np.asarray(num_labels))\n",
    "cw = dict(enumerate(cw.flatten(), 1))\n",
    "# since cw start at 1 instead of 0, we need to offset this by 1\n",
    "cw_z = dict()\n",
    "for key in cw:\n",
    "    cw_z[key-1] = cw[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082\n",
      "(9082, 299, 299, 3)\n"
     ]
    }
   ],
   "source": [
    "# loading train images\n",
    "trainPaths = list(paths.list_images('C:/Users/jdu12/Desktop/humpback/train/'))\n",
    "print(len(trainPaths))\n",
    "train = np.array([get_image(img_path) for img_path in trainPaths])\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and val\n",
    "# The dataset is super unbalanced, as there are many classes that contains only 1 image\n",
    "# As a result, train/val data cannot be split before generating more data by augmentation\n",
    "train_dir = 'C:/Users/jdu12/Desktop/humpback/train/'\n",
    "\n",
    "num_categories = len(df['Id'].unique())\n",
    "#print(num_categories)\n",
    "validation = np.zeros((num_categories, 224, 224, 3))\n",
    "validation_y = []\n",
    "\n",
    "i = 0\n",
    "for id in df['Id'].unique():\n",
    "    validation_y.append(id)\n",
    "    im = df[df['Id'] == id].sample(1)\n",
    "    name =  np.array(im.get('Image'))[0]\n",
    "    im = get_image(train_dir + name)\n",
    "\n",
    "    # https://www.kaggle.com/lextoumbourou/humpback-whale-id-data-and-aug-exploration\n",
    "    x = random.randint(0, 3)\n",
    "    if x == 0:\n",
    "        validation[i,:, :, :] = random_rotation(im, 30, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest')\n",
    "    if x == 1:\n",
    "        validation[i,:, :, :] = random_shift(im, wrg=0.1, hrg=0.3, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest')\n",
    "    if x == 2:\n",
    "        validation[i,:, :, :] = random_shear(im, intensity=0.4, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest')\n",
    "    if x == 3:\n",
    "        validation[i,:, :, :] = random_zoom(im, zoom_range=(1.5, 0.7), row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest')\n",
    "    i = i + 1\n",
    "\n",
    "validation_y = to_one_hot(validation_y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire the model\n",
    "model = keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=True, weights=None, classes=4251)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "283/283 [==============================] - 249s 879ms/step - loss: 8.3683 - acc: 0.0681\n",
      "Epoch 2/180\n",
      "283/283 [==============================] - 203s 719ms/step - loss: 7.9549 - acc: 0.0698\n",
      "Epoch 3/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 7.7267 - acc: 0.0701\n",
      "Epoch 4/180\n",
      "283/283 [==============================] - 203s 718ms/step - loss: 7.4033 - acc: 0.0698\n",
      "Epoch 5/180\n",
      "283/283 [==============================] - 203s 719ms/step - loss: 7.1138 - acc: 0.0694\n",
      "Epoch 6/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 6.9185 - acc: 0.0705\n",
      "Epoch 7/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 6.6631 - acc: 0.0698\n",
      "Epoch 8/180\n",
      "283/283 [==============================] - 203s 719ms/step - loss: 6.4601 - acc: 0.0688\n",
      "Epoch 9/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 6.3088 - acc: 0.0691\n",
      "Epoch 10/180\n",
      "283/283 [==============================] - 203s 719ms/step - loss: 6.1130 - acc: 0.0696\n",
      "Epoch 11/180\n",
      "283/283 [==============================] - 203s 719ms/step - loss: 5.9583 - acc: 0.0709\n",
      "Epoch 12/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 5.8375 - acc: 0.0664\n",
      "Epoch 13/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 5.6712 - acc: 0.0693\n",
      "Epoch 14/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 5.5207 - acc: 0.0660\n",
      "Epoch 15/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 5.3664 - acc: 0.0692\n",
      "Epoch 16/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 5.1731 - acc: 0.0702\n",
      "Epoch 17/180\n",
      "283/283 [==============================] - 203s 719ms/step - loss: 4.9855 - acc: 0.0759\n",
      "Epoch 18/180\n",
      "283/283 [==============================] - 203s 719ms/step - loss: 4.8570 - acc: 0.0813\n",
      "Epoch 19/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 4.6758 - acc: 0.0881\n",
      "Epoch 20/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 4.4777 - acc: 0.1048\n",
      "Epoch 21/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 4.3113 - acc: 0.1183\n",
      "Epoch 22/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 4.1102 - acc: 0.1374\n",
      "Epoch 23/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 3.9180 - acc: 0.1643\n",
      "Epoch 24/180\n",
      "283/283 [==============================] - 203s 719ms/step - loss: 3.7360 - acc: 0.1827\n",
      "Epoch 25/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 3.5460 - acc: 0.2117\n",
      "Epoch 26/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 3.3345 - acc: 0.2452\n",
      "Epoch 27/180\n",
      "283/283 [==============================] - 203s 719ms/step - loss: 3.0773 - acc: 0.2868\n",
      "Epoch 28/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 2.8884 - acc: 0.3201\n",
      "Epoch 29/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 2.6482 - acc: 0.3605\n",
      "Epoch 30/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 2.4403 - acc: 0.4020\n",
      "Epoch 31/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 2.2530 - acc: 0.4424\n",
      "Epoch 32/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 2.0790 - acc: 0.4729\n",
      "Epoch 33/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 1.8933 - acc: 0.5234\n",
      "Epoch 34/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 1.7372 - acc: 0.5591\n",
      "Epoch 35/180\n",
      "283/283 [==============================] - 203s 719ms/step - loss: 1.5768 - acc: 0.5935\n",
      "Epoch 36/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 1.4496 - acc: 0.6227\n",
      "Epoch 37/180\n",
      "283/283 [==============================] - 203s 719ms/step - loss: 1.2741 - acc: 0.6720\n",
      "Epoch 38/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 1.1773 - acc: 0.6951\n",
      "Epoch 39/180\n",
      "283/283 [==============================] - 203s 719ms/step - loss: 1.0611 - acc: 0.7232\n",
      "Epoch 40/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.9575 - acc: 0.7382\n",
      "Epoch 41/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.8458 - acc: 0.7711\n",
      "Epoch 42/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.7767 - acc: 0.7929\n",
      "Epoch 43/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 0.6757 - acc: 0.8185\n",
      "Epoch 44/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.6018 - acc: 0.8409\n",
      "Epoch 45/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.5682 - acc: 0.8430\n",
      "Epoch 46/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.5122 - acc: 0.8624\n",
      "Epoch 47/180\n",
      "283/283 [==============================] - 203s 719ms/step - loss: 0.4552 - acc: 0.8754\n",
      "Epoch 48/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.4160 - acc: 0.8924\n",
      "Epoch 49/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.3822 - acc: 0.8961\n",
      "Epoch 50/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 0.3548 - acc: 0.9072\n",
      "Epoch 51/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 0.3077 - acc: 0.9201\n",
      "Epoch 52/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.3159 - acc: 0.9095\n",
      "Epoch 53/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 0.2811 - acc: 0.9221\n",
      "Epoch 54/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.2585 - acc: 0.9275\n",
      "Epoch 55/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 0.2396 - acc: 0.9365\n",
      "Epoch 56/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 0.2186 - acc: 0.9429\n",
      "Epoch 57/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.2004 - acc: 0.9456\n",
      "Epoch 58/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.2101 - acc: 0.9424\n",
      "Epoch 59/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.2048 - acc: 0.9442\n",
      "Epoch 60/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.1915 - acc: 0.9481\n",
      "Epoch 61/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.1920 - acc: 0.9470\n",
      "Epoch 62/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.1963 - acc: 0.9420\n",
      "Epoch 63/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.1786 - acc: 0.9493\n",
      "Epoch 64/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.1461 - acc: 0.9621\n",
      "Epoch 65/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.1356 - acc: 0.9623\n",
      "Epoch 66/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.1500 - acc: 0.9593\n",
      "Epoch 67/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.1375 - acc: 0.9608\n",
      "Epoch 68/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.1339 - acc: 0.9628\n",
      "Epoch 69/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.1258 - acc: 0.9645\n",
      "Epoch 70/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.1327 - acc: 0.9617\n",
      "Epoch 71/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.1227 - acc: 0.9678\n",
      "Epoch 72/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.1294 - acc: 0.9653\n",
      "Epoch 73/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 0.1145 - acc: 0.9682\n",
      "Epoch 74/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.1143 - acc: 0.9706\n",
      "Epoch 75/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.1193 - acc: 0.9668\n",
      "Epoch 76/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.1023 - acc: 0.9714\n",
      "Epoch 77/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.1056 - acc: 0.9710\n",
      "Epoch 78/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0792 - acc: 0.9800\n",
      "Epoch 79/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0846 - acc: 0.9772\n",
      "Epoch 80/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.1080 - acc: 0.9688\n",
      "Epoch 81/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0915 - acc: 0.9767\n",
      "Epoch 82/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0695 - acc: 0.9816\n",
      "Epoch 83/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.1004 - acc: 0.9714\n",
      "Epoch 84/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0948 - acc: 0.9726\n",
      "Epoch 85/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0796 - acc: 0.9791\n",
      "Epoch 86/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0704 - acc: 0.9806\n",
      "Epoch 87/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0932 - acc: 0.9731\n",
      "Epoch 88/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.1115 - acc: 0.9671\n",
      "Epoch 89/180\n",
      "283/283 [==============================] - 203s 718ms/step - loss: 0.0655 - acc: 0.9823\n",
      "Epoch 90/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0515 - acc: 0.9874\n",
      "Epoch 91/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0645 - acc: 0.9819\n",
      "Epoch 92/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 0.0583 - acc: 0.9840\n",
      "Epoch 93/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0773 - acc: 0.9788\n",
      "Epoch 94/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 0.0657 - acc: 0.9826\n",
      "Epoch 95/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 0.0933 - acc: 0.9756\n",
      "Epoch 96/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0700 - acc: 0.9811\n",
      "Epoch 97/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0731 - acc: 0.9791\n",
      "Epoch 98/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0591 - acc: 0.9835\n",
      "Epoch 99/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 0.0598 - acc: 0.9813\n",
      "Epoch 100/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0651 - acc: 0.9808\n",
      "Epoch 101/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0813 - acc: 0.9779\n",
      "Epoch 102/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0554 - acc: 0.9851\n",
      "Epoch 103/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0436 - acc: 0.9896\n",
      "Epoch 104/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 0.0463 - acc: 0.9883\n",
      "Epoch 105/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0608 - acc: 0.9836\n",
      "Epoch 106/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0607 - acc: 0.9840\n",
      "Epoch 107/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0545 - acc: 0.9856\n",
      "Epoch 108/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0476 - acc: 0.9864\n",
      "Epoch 109/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0602 - acc: 0.9832\n",
      "Epoch 110/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0524 - acc: 0.9861\n",
      "Epoch 111/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0539 - acc: 0.9859\n",
      "Epoch 112/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0566 - acc: 0.9843\n",
      "Epoch 113/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0521 - acc: 0.9845\n",
      "Epoch 114/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0333 - acc: 0.9912\n",
      "Epoch 115/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0423 - acc: 0.9895\n",
      "Epoch 116/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0461 - acc: 0.9879\n",
      "Epoch 117/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0561 - acc: 0.9845\n",
      "Epoch 118/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0443 - acc: 0.9862\n",
      "Epoch 119/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0502 - acc: 0.9865\n",
      "Epoch 120/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0482 - acc: 0.9866\n",
      "Epoch 121/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0354 - acc: 0.9904\n",
      "Epoch 122/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 0.0329 - acc: 0.9913\n",
      "Epoch 123/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0311 - acc: 0.9906\n",
      "Epoch 124/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0557 - acc: 0.9835\n",
      "Epoch 125/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0503 - acc: 0.9869\n",
      "Epoch 126/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0418 - acc: 0.9871\n",
      "Epoch 127/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0416 - acc: 0.9890\n",
      "Epoch 128/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0457 - acc: 0.9864\n",
      "Epoch 129/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0283 - acc: 0.9919\n",
      "Epoch 130/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0311 - acc: 0.9913\n",
      "Epoch 131/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0362 - acc: 0.9893\n",
      "Epoch 132/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0421 - acc: 0.9876\n",
      "Epoch 133/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0418 - acc: 0.9886\n",
      "Epoch 134/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0337 - acc: 0.9892\n",
      "Epoch 135/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0408 - acc: 0.9887\n",
      "Epoch 136/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0462 - acc: 0.9852\n",
      "Epoch 137/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0314 - acc: 0.9905\n",
      "Epoch 138/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0366 - acc: 0.9888\n",
      "Epoch 139/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0280 - acc: 0.9926\n",
      "Epoch 140/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0224 - acc: 0.9934\n",
      "Epoch 141/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0260 - acc: 0.9932\n",
      "Epoch 142/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0343 - acc: 0.9902\n",
      "Epoch 143/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0222 - acc: 0.9933\n",
      "Epoch 144/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0402 - acc: 0.9882\n",
      "Epoch 145/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0353 - acc: 0.9900\n",
      "Epoch 146/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0438 - acc: 0.9878\n",
      "Epoch 147/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0336 - acc: 0.9898\n",
      "Epoch 148/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0320 - acc: 0.9907\n",
      "Epoch 149/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0314 - acc: 0.9924\n",
      "Epoch 150/180\n",
      "283/283 [==============================] - 204s 719ms/step - loss: 0.0261 - acc: 0.9922\n",
      "Epoch 151/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0319 - acc: 0.9905\n",
      "Epoch 152/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0280 - acc: 0.9917\n",
      "Epoch 153/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0308 - acc: 0.9909\n",
      "Epoch 154/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0253 - acc: 0.9933\n",
      "Epoch 155/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0168 - acc: 0.9945\n",
      "Epoch 156/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0211 - acc: 0.9938\n",
      "Epoch 157/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0252 - acc: 0.9926\n",
      "Epoch 158/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0246 - acc: 0.9934\n",
      "Epoch 159/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0456 - acc: 0.9862\n",
      "Epoch 160/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0350 - acc: 0.9883\n",
      "Epoch 161/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0230 - acc: 0.9934\n",
      "Epoch 162/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0420 - acc: 0.9888\n",
      "Epoch 163/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0260 - acc: 0.9923\n",
      "Epoch 164/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0210 - acc: 0.9947\n",
      "Epoch 165/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0136 - acc: 0.9953\n",
      "Epoch 166/180\n",
      "283/283 [==============================] - 217s 765ms/step - loss: 0.0094 - acc: 0.9964\n",
      "Epoch 167/180\n",
      "283/283 [==============================] - 207s 731ms/step - loss: 0.0114 - acc: 0.9962\n",
      "Epoch 168/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0251 - acc: 0.9922\n",
      "Epoch 169/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0415 - acc: 0.9873\n",
      "Epoch 170/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0387 - acc: 0.9888\n",
      "Epoch 171/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0168 - acc: 0.9945\n",
      "Epoch 172/180\n",
      "283/283 [==============================] - 203s 719ms/step - loss: 0.0149 - acc: 0.9953\n",
      "Epoch 173/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0186 - acc: 0.9948\n",
      "Epoch 174/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0241 - acc: 0.9926\n",
      "Epoch 175/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0230 - acc: 0.9929\n",
      "Epoch 176/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0185 - acc: 0.9946\n",
      "Epoch 177/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0208 - acc: 0.9932\n",
      "Epoch 178/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0224 - acc: 0.9928\n",
      "Epoch 179/180\n",
      "283/283 [==============================] - 204s 721ms/step - loss: 0.0324 - acc: 0.9917\n",
      "Epoch 180/180\n",
      "283/283 [==============================] - 204s 720ms/step - loss: 0.0294 - acc: 0.9921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2646d0ba550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 200\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_dir = 'C:/Users/jdu12/Desktop/humpback/saved_model/InceptionResnetV2/'\n",
    "routine_dir = model_dir + \"routine-{epoch:02d}-{acc:.2f}.hdf5\"\n",
    "routine_save = ModelCheckpoint(routine_dir, monitor='acc', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=10)\n",
    "best_dir = model_dir + \"best-{epoch:02d}-{acc:.2f}.hdf5\"\n",
    "best_save = ModelCheckpoint(best_dir, monitor='acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=3)\n",
    "\n",
    "\n",
    "hist = model.fit_generator(image_gen.flow(train, labels, batch_size=batch_size),\n",
    "                           steps_per_epoch=  train.shape[0]//batch_size,\n",
    "                           epochs=epochs, verbose=1, callbacks=[routine_save, best_save], class_weight=cw_z,\n",
    "                           validation_data=(validation, validation_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15610\n",
      "(15610, 299, 299, 3)\n"
     ]
    }
   ],
   "source": [
    "# loading test images\n",
    "testPaths = list(paths.list_images('C:/Users/jdu12/Desktop/humpback/test/'))\n",
    "print(len(testPaths))\n",
    "test_images = np.array([get_image(img_path) for img_path in testPaths])\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading trained model\n",
    "model_name = 'routine-130-0.99'\n",
    "model = load_model('C:/Users/jdu12/Desktop/humpback/saved_model/InceptionResnetV2/' + model_name+ '.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict!\n",
    "pred = model.predict(test_images)\n",
    "\n",
    "# testing and generating submission file\n",
    "import warnings\n",
    "from os.path import split\n",
    "\n",
    "pred_dir = \"C:/Users/jdu12/Desktop/humpback/prediction/InceptionResnetV2/\"\n",
    "\n",
    "with open(pred_dir + model_name + \".csv\", \"w\") as f:\n",
    "    f.write(\"Image,Id\\n\")\n",
    "    top_5 = np.argsort(pred)[:,-1:-6:-1]   # get the top 5 most likely classes\n",
    "    for i in range(top_5.shape[0]):\n",
    "        cur_tags = ''\n",
    "        cur_image_name = testPaths[i].split('/')[-1]\n",
    "        for j in range(5):\n",
    "            cur_tags = cur_tags + ' ' + class2id[top_5[i][j]]\n",
    "        f.write(\"%s,%s\\n\" %(cur_image_name, cur_tags))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
